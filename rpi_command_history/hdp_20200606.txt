    1  exit
    2  ls -al
    3  hadoop ffs -la
    4  hadoop fs -ls
    5  hadoop fs -mkdir ml-100k
    6  hadoop fs -ls 
    7  ls -al
    8  pwd
    9  ls -al
   10  mkdir Share
   11  cd Share
   12  ls -al
   13  wget media.sundog-soft.com/hadoop/ml-100k/u.data
   14  ls -al
   15  ls-al
   16  ls -al
   17  hadoop fs -copyFromLocal u.data ml-100k/u.data
   18  hadoop fs -al 
   19  hadoop fs -ls 
   20  hadoop fs -ls ml-100k
   21  hadoop fs -rm ml-100k/u.data
   22  hadoop fs -rmdir ml-100k
   23  uname -a 
   24  lsb_release -a
   25  hadoop fs -sl
   26  sl
   27  hadoop fs -ls 
   28  exit
   29  ls -al
   30  hadoop fs -ls 
   31  python3
   32  pip3 install
   33  python
   34  pip install mrjob==0.5.11
   35  pip3 install mrjob==0.5.11
   36  yum install python-pip
   37  sudo yum install python-pip
   38  pip install mrjon==0.5.11
   39  pip install --upgrade pip
   40  sudo pip install --upgrade pip
   41  pip install mrjob==0.5.11
   42  su root
   43  nano 
   44  yum install nano
   45  sudo yum install nano 
   46  nano
   47  wget media.sundog-soft.com/hadoop/RatingsBreakdown.py
   48  nano RatingsBreakdown.py 
   49  ls -al 
   50  nano RatingsBreakdown.py 
   51  cd Share/
   52  ls -al
   53  cp u.data ../
   54  cd .
   55  cd ..
   56  ls-al
   57  ls -al
   58  python RatingsBreakdown.py  u.data 
   59  python RatingsBreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar u.data 
   60  cp RatingsBreakdown.py myRatingsBreakdown.py 
   61  nano myRatingsBreakdown.py 
   62  python  myRatingsBreakdown.py  u.data 
   63  nano myRatingsBreakdown.py 
   64  python  myRatingsBreakdown.py  u.data 
   65  nano myRatingsBreakdown.py 
   66  ls -al
   67  python  myRatingsBreakdown.py  u.data 
   68  nano myRatingsBreakdown.py 
   69  python  myRatingsBreakdown.py  u.data 
   70  nano myRatingsBreakdown.py 
   71  python  myRatingsBreakdown.py  u.data 
   72  exit
   73  sudo ambari-admin-password-reset
   74  exit
   75  pig
   76  ls -al 
   77  nano myRatingsBreakdown.py 
   78  exit
   79  pig
   80  ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS(userID:int, movieID: int, rating:int, ratingTime:int);
   81  pig
   82  sc
   83  ls
   84  mkdir ml-100k
   85  cd m 
   86  cd ml-100k/ 
   87  ls -al
   88  ls -al ../Share/
   89  wget media.sundog-soft.com/hadoop/ml-100k/u.item
   90  ls -al
   91  cd ..
   92  ls -al
   93  ls 
   94  wget media.sundog-soft.com/hadoop/Spark.zip
   95  unzip Spark.zip
   96  ls -al
   97  less LowestRatedMovieSpark.py 
   98  spark-submit LowestRatedMovieSpark.py 
   99  export SPARK_MAJOR_VERSION=2
  100  spark-submit LowestRatedMovieDataFrame.py 
  101  ls -al
  102  nano u.data 
  103  spark-submit MovieRecommendationsALS.py
  104  export SPARK_MAJOR_VERSION=2
  105  spark-submit MovieRecommendationsALS.py
  106  sudo pip install numpy
  107  spark-submit MovieRecommendationsALS.py
  108  ls -al
  109  cat MovieRecommendationsALS.py
  110  export SPARK_MAJOR_VERSION=2
  111  spark-submit LowestRatedPopularMovieDataFrame.py 
  112  mysql -uroot
  113  mysql -uroot -p
  114  wget media.sundog-soft.com/hadoop/movielens.sql
  115  ls -al
  116  cat movielens.sql
  117  mysql -uroot -p
  118  sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1 
  119  mysql -uroot -p
  120  sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --username root --password hortonworks1 --table movies -m 1 
  121  ls -al
  122  sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --username root --password hortonworks1 --table movies -m 1 --hive-import
  123  mysql -uroot -p
  124  sqoop export --connect jdbc:/mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movirs --export-dir /apps/hives/warehouse/movies --input-fields-terminated-by '\0001';
  125  sqoop export --connect jdbc://mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movirs --export-dir /apps/hives/warehouse/movies --input-fields-terminated-by '\0001';
  126  sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movirs --export-dir /apps/hives/warehouse/movies --input-fields-terminated-by '\0001' --username root --password xxxxxxxxxx ;
  127  sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movirs --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001' --username root --password xxxxxxxxxx ;
  128  mysql -uroot -p
  129  su root
  130  su
  131  su root
  132  sudo service --status-all
  133  sudo service Ambari-server restart
  134  sudo ambari-server restart
  135  pip install pymongo
  136  cd ~ 
  137  pwd
  138  ls -al 
  139  wget media.sundog-soft.com/hadoop/MongoSpark.py
  140  ls a-l
  141  ls -al
  142  export SPARK_MAJOR_VERSION=2
  143  spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.11:2.0.0 MongoSpark.py 
  144  mongo
  145  su root
  146  history
  147  ls -al
  148  cd apache-drill-1.12.0
  149  sudo bin/drillbit.sh -Ddrill.exec.http.port=8765
  150  sudo bin/drillbit.sh restart -Ddrill.exec.http.port=8765
  151  sudo nano /etc/ssh/ssh_config
  152  ls -al
  153  cd apache-drill-1.12.0
  154  ls -al
  155  sudo bin/drillbit.sh  stop
  156  sudo nano /etc/ssh/sshd_config
  157  sudo service sshd restart
  158  sudo nano /etc/ssh/sshd_config
  159  sudo service sshd restart
  160  su root
  161  history
  162  mysql -uroot -p
  163  wget media.sundog-soft.com/hadoop/oldmovies.sql
  164  less oldmovies.sql  
  165  wget media.sundog-soft.com/hadoop/workflow.xml
  166  cat workflow.xml 
  167  wget media.sundog-soft.com/hadoop/job.properties
  168  hadoop fs -put workflow.xml /user/maria_dev
  169  hadoop fs -put oldmovies.sql /user/maria_dev
  170  hadoop fs -put /usr/share/java/mysql-connector-java.jar /user/oozie/share/lib/lib_20180618160835/sqoop 
  171  oozie job --oozie http://localhost:11000/oozie -config /home/maria_dev/job.properties -run
  172  exit
  173  cd /usr/hdp/current/kafka-broker/
  174  ls 
  175  cd bin/
  176  ls -al
  177  ./kafka-topics.sh --create --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partition 1 --topic fred
  178  ./kafka-topics.sh --create --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partitions 1 --topic fred
  179  ./kafka-topics.sh --create --zookeeper sandbox-hdp.hortonworks.com:2181 --replication-factor 1 --partitions 1 --topic fred
  180  ./kafka-topics.sh --list --zookeeper sandbox-hdp.hortonworks.com:2181
  181  ./kafka-console-producer.sh --broker-list sandbox-hdp.hortonworks.com:6667 --topic fred
  182  pwd
  183  ./kafka-console-consumer.sh --topic log-test --zookeeper  localhost:2181
  184  exit
  185  exit 
  186  cd /usr/hdp/current/kafka-broker/bin/
  187  ./kafka-console-consumer.sh --bootstrap-server sandbox-hdp.hortonworks.com:6667 --zookeeper localhost:2181 --topic fred --from-begining
  188  ./kafka-console-consumer.sh --bootstrap-server sandbox-hdp.hortonworks.com:6667 --zookeeper localhost:2181 --topic fred --from-beginning
  189  ./kafka-console-consumer.sh --zookeeper localhost:2181 --topic fred --from-beginning
  190  cd ..
  191  cd conf
  192  ls -al
  193  cp connect-standalone.properties ~
  194  cp connect-file-sink.properties ~ 
  195  cp connect-file-source.properties ~
  196  cd ~ 
  197  ls -al
  198  nano connect-standalone.properties 
  199  nano connect-file-sink.properties
  200  nano connect-file-source.properties 
  201  wget media.sundog-soft.com/hadoop/access_log_small.txt
  202  less access_log_small.txt 
  203  pwd
  204  cd /usr/hdp/current/kafka-broker/bin/
  205  ls -al
  206  ./connect-standalone.sh ~/connect-file-sink.properties ~/connect-file-source.properties ~/connect-standalone.properties 
  207  cd ~ 
  208  cd ~
  209  nano connect-standalone.properties 
  210  ./connect-standalone.sh ~/connect-file-sink.properties ~/connect-file-source.properties
  211  cd /usr/hdp/current/kafka-broker/bin/
  212  ./connect-standalone.sh ~/connect-file-sink.properties ~/connect-file-source.properties
  213  cd /usr/hdp/current/flume-server/
  214  bin/flume-ng agent --conf conf --conf-file ~/example.conf --name a1 --Dflume.rot.logger=Info,console
  215  sudo bin/flume-ng agent --conf conf --conf-file ~/example.conf --name a1 --Dflume.rot.logger=Info,console
  216  sudo bin/flume-ng agent --conf conf --conf-file /home/maria_dev/example.conf --name a1 --Dflume.rot.logger=Info,console
  217  bin/flume-ng agent --conf conf --conf-file /home/maria_dev/example.conf --name a1 --Dflume.rot.logger=Info,console
  218  sudo chmod 777 /var/log/flume/flume.log
  219  bin/flume-ng agent --conf conf --conf-file /home/maria_dev/example.conf --name a1 --Dflume.rot.logger=Info,console
  220  exit
  221  history
  222  wget media.sundog-soft.com/hadoop/example.conf
  223  nano sample-data/
  224  nano example.conf 
  225  wget media.sundog-soft.com/hadoop/flumelogs.conf
  226  less flumelogs.conf  
  227  mkdir spool
  228  history
  229  cd /usr/
  230  ls -al
  231  cd hdp/current/
  232  ls 
  233  cd flume-server/
  234  bin/flume-ng agent --conf conf --conf-file ~/flumelogs.conf --name a1 -Dflume.root.logger=INFO,console
  235  exit
  236  wget media.sundog-soft.com/hadoop/sparkstreamingflume.conf
  237  wget media.sundog-soft.com/hadoop/SparkFlume.py
  238  mkdir checkpoint
  239  export SPARK_MAJOR_VERSION=2
  240  ls -al
  241  spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py 
  242  cd /usr/hdp/current/
  243  cd storm-client/
  244  cd contrib/storm-starter/src/jvm/org/apache/storm/
  245  ls
  246  cd starter/ 
  247  pwd
  248  ls 
  249  storm jar /usr/hdp/current/storm-client/contrib/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.WordCountTopology wordcount
  250  cd flink-1.2.1
  251  cd log/
  252  ls -al
  253  cat flink-maria_dev-jobmanager-0-sandbox-hdp.hortonworks.com.out
  254  exit
  255  cd flink-1.2.1
  256  ./bin/flink run examples/steaming/SocketWindowWordCount.jar -- port 9000
  257  ls -al
  258  ./bin/flink run examples/streaming/SocketWindowWordCount.jar -- port 9000
  259  ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000
  260  exit
  261  wget https://apache.cs.utah.edu/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz
  262  wget http://apache.mirrors.hoobly.com/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz
  263  ls -al
  264  rm flink-1.10.1-bin-scala_2.11.tgz
  265  tar zxvf flink-1.10.1-bin-scala_2.11.tgz.1
  266  cd flink-1.10.1
  267  ls 
  268  cd conf/
  269  nano flink-conf.yaml 
  270  cd ..
  271  ls -al
  272  cd bin/ 
  273  ls -al
  274  cd ..
  275   ls -al
  276  sudo rm -rf flink-1.10.1
  277  sudo rm flink-1.10.1-bin-scala_2.11.tgz.1
  278  wget https://archive.apache.org/dist/flink/flink-1.2.1/flink-1.2.1-bin-hadoop27-scala_2.10.tgz
  279  tar zxvf flink-1.2.1-bin-hadoop27-scala_2.10.tgz
  280  cd flink-1.2.1
  281  ls -al
  282  cd conf
  283  ls -al
  284  nanonflink-conf.yaml
  285  nano flink-conf.yaml
  286  cd ..
  287   ls -al
  288  ./bin/start-local.sh 
  289  nc -l 9000
  290  history > hdp_20200606.txt
